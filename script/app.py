# script/app.py
from flask import Flask, request, render_template
from markupsafe import Markup
import requests, random, re, traceback
from bs4 import BeautifulSoup
from pathlib import Path
from pythainlp.util import normalize
import json

# --- PyThaiNLP: ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ---
from pythainlp.summarize import summarize
from pythainlp.tokenize import sent_tokenize

# --- HuggingFace transformers: NER ---
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

# -------------------------------------------------
# ‡∏ä‡∏µ‡πâ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå templates = TRAIN_AI/web  ‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
# -------------------------------------------------
THIS_DIR = Path(__file__).resolve().parent          # .../TRAIN_AI/script
ROOT_DIR = THIS_DIR.parent                           # .../TRAIN_AI
TPL_DIR  = ROOT_DIR / "web"                          # .../TRAIN_AI/web

app = Flask(__name__, template_folder=str(TPL_DIR))

# -------------------------------------------------
# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ HTTP headers ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡∏î‡∏∂‡∏á/‡∏Ñ‡∏•‡∏µ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß
# -------------------------------------------------
UA = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)",
    "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0)",
]

def clean_spaces(t: str) -> str:
    return re.sub(r"\s+", " ", t or "").strip()

def fetch_full(url: str) -> str:
    """‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå ‡∏û‡∏£‡πâ‡∏≠‡∏° selector ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö"""
    r = requests.get(url, timeout=12, headers={"User-Agent": random.choice(UA)})
    r.raise_for_status()
    r.encoding = r.apparent_encoding
    soup = BeautifulSoup(r.text, "html.parser")

    selectors = [
        "article",
        "div[itemprop='articleBody']",
        "div.entry-content",
        "div#article-body",
        "section.article",
        "div.td-post-content",
        "div#main-content",
        "div.content-detail",
        "div.post-content",
    ]
    for sel in selectors:
        el = soup.select_one(sel)
        if el:
            text = clean_spaces(el.get_text(" "))
            if len(text) > 200:
                return text
    return clean_spaces(soup.get_text(" "))

# -------------------------------------------------
# ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏ö‡∏ö‡πÑ‡∏ó‡∏¢ (TextRank) + ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Å‡∏£‡∏ì‡∏µ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß
# -------------------------------------------------
def summarize_th(text: str, n_sent: int = 5) -> str:
    try:
        s = summarize(text, n_sentences=n_sent)
        if s:
            return s
    except Exception:
        pass
    sents = sent_tokenize(text)
    return " ".join(sents[:n_sent])

def preprocess_for_inference(text: str) -> str:
    import re
    text = text.replace("‚Äú", "\"").replace("‚Äù", "\"").replace("‚Äò", "'").replace("‚Äô", "'")
    text = re.sub(r"\bSpace\s*X\b", "SpaceX", text, flags=re.I)
    text = re.sub(r"\s{2,}", " ", text)
    return text.strip()

# -------------------------------------------------
# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• NER (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡∏≠‡∏ô‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó‡πÅ‡∏≠‡∏õ)
# -------------------------------------------------
NER_MODEL = "pythainlp/thainer-corpus-v2-base-model"
ner_tokenizer = AutoTokenizer.from_pretrained(NER_MODEL)
ner_model = AutoModelForTokenClassification.from_pretrained(NER_MODEL)
ner = pipeline("ner", model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy="simple")

LABEL_COLOR = {
    "PERSON": "#b3d9ff",
    "ORGANIZATION": "#ffd1b3",
    "LOCATION": "#c2f0c2",
    "DATE": "#ffe680",
    "TIME": "#ffd6e7",
    "MONEY": "#e6ccff",
    "PERCENT": "#e0ffff",
    "LAW": "#ddd",
}

import unicodedata

def preprocess_for_inference(text: str) -> str:
    # ‡∏£‡∏ß‡∏° Space X -> SpaceX ‡∏Å‡∏±‡∏ô‡∏´‡∏•‡∏∏‡∏î‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß
    text = re.sub(r"\bSpace\s*X\b", "SpaceX", text, flags=re.I)
    # ‡∏•‡∏ö‡∏≠‡∏±‡∏ç‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®/‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏≠‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏ï‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠
    text = text.replace("‚Äú", "\"").replace("‚Äù", "\"").replace("‚Äò", "'").replace("‚Äô", "'")
    text = unicodedata.normalize("NFC", text)
    # ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥
    text = re.sub(r"\s{2,}", " ", text)
    return text.strip()

import unicodedata

THAI_DIGITS = str.maketrans("‡πê‡πë‡πí‡πì‡πî‡πï‡πñ‡πó‡πò‡πô","0123456789")

def thai_to_arabic(s: str) -> str:
    return s.translate(THAI_DIGITS)

def extend_date_year_span(text: str, start: int, end: int) -> int:
    frag = text[start:end]
    # ‡∏à‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏•‡∏Ç 2 ‡∏´‡∏•‡∏±‡∏Å (‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ó‡∏¢)
    two_tail = re.search(r"([0-9]{2}|[‡πê-‡πô]{2})\s*$", frag)
    if not two_tail:
        return end

    i = end
    while i < len(text) and text[i] in (" ", "\t", "\u00a0", "\u200b", ".", ",", "‚Äì", "-", "‚Ä¢"):
        i += 1

    # ‡∏°‡∏≠‡∏á‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏≠‡∏µ‡∏Å 2 ‡∏´‡∏•‡∏±‡∏Å‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    m = re.match(r"([0-9]{2}|[‡πê-‡πô]{2})", text[i:])
    if m:
        return i + len(m.group(0))
    return end

STOP_ENTS = {
    "‡πÉ‡∏ô","‡∏Ç‡∏≠‡∏á","‡∏ó‡∏µ‡πà","‡πÅ‡∏•‡∏∞","‡∏´‡∏£‡∏∑‡∏≠","‡∏Ø","‡∏Ø‡∏•‡∏Ø","‡∏Ç‡πà‡∏≤‡∏ß","‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏Ç‡πà‡∏≤‡∏ß","‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ","‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô",
    "‡∏ú‡∏π‡πâ‡∏™‡∏∑‡πà‡∏≠‡∏Ç‡πà‡∏≤‡∏ß","‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô","‡∏†‡∏≤‡∏û","‡∏Ñ‡∏•‡∏¥‡∏õ"
}
STOP_DATE_WORDS = {"‡∏™‡∏¥‡πâ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô","‡∏ï‡πâ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô","‡∏Å‡∏•‡∏≤‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô","‡∏õ‡∏•‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô","‡∏ï.‡∏Ñ."}

LABEL_COLOR = {
    "PERSON": "#b3d9ff","ORGANIZATION": "#ffd1b3","LOCATION": "#c2f0c2",
    "DATE": "#ffe680","TIME": "#ffd6e7","MONEY": "#e6ccff","PERCENT": "#e0ffff","LAW": "#ddd",
}

def normalize_ent_view(s: str) -> str:
    # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÇ‡∏ä‡∏ß‡πå‡∏™‡∏ß‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô (‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÉ‡∏ô‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö)
    s = s.strip().strip('"\''"()[] ")
    s = re.sub(r"\s{2,}", " ", s)
    return thai_to_arabic(s)

def highlight_entities(text: str):
    raw = ner(text)  # [{'start','end','word','entity_group','score'}]
    spans = []
    total_score = 0.0
    total_entity = 0  

    for r in raw:
        lab = r.get("entity_group") or r.get("entity") or "O"
        start, end = int(r.get("start", -1)), int(r.get("end", -1))
        word = (r.get("word") or "").strip()
        score = float(r.get("score", 0.0))  # ‚úÖ ‡∏î‡∏∂‡∏á score
        total_score += score
        total_entity += 1

        if lab == "O" or not word or start < 0 or end <= start:
            continue

        # ‡∏Ç‡∏¢‡∏≤‡∏¢ DATE
        if lab == "DATE":
            end = extend_date_year_span(text, start, end)
            word = text[start:end].strip()

        if len(word) < 2 or word in STOP_ENTS:
            continue
        if lab == "DATE" and word in STOP_DATE_WORDS:
            continue
        if re.fullmatch(r"[0-9,./:-]+", word):
            continue
        if word == "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó":
            continue

        spans.append({
            "start": start,
            "end": end,
            "label": lab,
            "word": word,
            "score": score
        })

    # üî∏ ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏ô‡∏ó‡∏¥‡∏ï‡∏µ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
    pruned = []
    for i, a in enumerate(spans):
        contained = False
        for j, b in enumerate(spans):
            if i != j and a["label"] == b["label"] and b["start"] <= a["start"] and b["end"] >= a["end"]:
                if (b["end"] - b["start"]) > (a["end"] - a["start"]):
                    contained = True
                    break
        if not contained:
            pruned.append(a)
    spans = pruned

    # ‚úÖ ‡πÉ‡∏™‡πà mark
    spans.sort(key=lambda s: s["start"], reverse=True)
    html = text
    ent_table = {}

    for sp in spans:
        frag = html[sp["start"]:sp["end"]]
        color = LABEL_COLOR.get(sp["label"], "#f2f2f2")
        marked = f"<mark class='ent' style='background:{color}' title='{sp['label']} ({sp['score']:.2f})'>{frag}</mark>"
        html = html[:sp["start"]] + marked + html[sp["start"]+len(frag):]

        # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô dict ‡∏û‡∏£‡πâ‡∏≠‡∏° score
        ent_table.setdefault(sp["label"], []).append({
            "word": frag,
            "score": sp["score"]
        })
        
    avg_score = total_score / total_entity if total_entity > 0 else 0.0

    return Markup(html), ent_table, round(avg_score, 2)

# -------------------------------------------------
# Kuy Clean
# -------------------------------------------------

def clean_text(txt: str) -> str:
    """‡∏•‡πâ‡∏≤‡∏á HTML, ‡∏≠‡∏µ‡πÇ‡∏°‡∏à‡∏¥, ‡∏Ç‡∏¢‡∏∞‡∏ó‡πâ‡∏≤‡∏¢‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥"""
    if not txt:
        return ""

    # 1) ‡πÄ‡∏≠‡∏≤‡πÅ‡∏ó‡πá‡∏Å HTML ‡∏≠‡∏≠‡∏Å
    txt = re.sub(r"<[^>]+>", " ", txt)

    # 2) ‡∏•‡∏ö emoji ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ô‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á Unicode ‡πÑ‡∏ó‡∏¢/‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
    txt = re.sub(r"[\U00010000-\U0010ffff]", "", txt)

    # 3) ‡∏•‡∏ö‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß
    txt = re.sub(r"‡∏û‡∏¥‡∏°‡∏û‡πå ‡πÅ‡∏ä‡∏£‡πå‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ ‡πÅ‡∏ä‡∏£‡πå‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ Line Twitter Facebook ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå - ‡∏Å ‡∏Å", "", txt)
    txt = re.sub(r"(‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡∏ó‡∏µ‡πà.*|‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ä‡∏°‡∏†‡∏≤‡∏û.*|‡∏î‡∏π‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°.*|‡πÄ‡∏Ñ‡∏£‡∏î‡∏¥‡∏ï‡∏†‡∏≤‡∏û.*)", "", txt)
    txt = re.sub(r"(appeared first on .*|The post .*)", "", txt, flags=re.I)
    txt = re.sub(r"(Facebook.*?Twitter.*?LINE)", "", txt)
    txt = re.sub(r"&#82\d{2};", "", txt)
    txt = re.sub(r"\s*\[\]\s*", " ", txt)
    txt = re.sub(r"‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö:.*$", "", txt)
    txt = re.sub(r"Thairath Online Thairath Money Thairath Shopping Thairath Plus Thairath TV MIRROR ‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å ‡∏ü‡∏∏‡∏ï‡∏ö‡∏≠‡∏•‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® ‡∏û‡∏£‡∏µ‡πÄ‡∏°‡∏µ‡∏¢‡∏£‡πå‡∏•‡∏µ‡∏Å ‡∏¢‡∏π‡∏ü‡πà‡∏≤ ‡πÅ‡∏ä‡∏°‡πÄ‡∏õ‡∏µ‡∏¢‡∏ô‡∏™‡πå‡∏•‡∏µ‡∏Å ‡∏ö‡∏∏‡∏ô‡πÄ‡∏î‡∏™‡∏•‡∏µ‡∏Å‡∏≤ ‡∏Å‡∏±‡∏•‡πÇ‡∏ä ‡πÄ‡∏ã‡πÄ‡∏£‡∏µ‡∏¢ ‡∏≠‡∏≤ ‡∏•‡∏≤ ‡∏•‡∏µ‡∏Å‡∏≤ ‡πÄ‡∏à‡∏•‡∏µ‡∏Å ‡∏•‡∏µ‡∏Å‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô", "", txt)
    txt = re.sub(r"‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°/‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô ‡∏î‡∏≤‡∏ß‡∏ã‡∏±‡∏•‡πÇ‡∏ß ‡∏ü‡∏∏‡∏ï‡∏ö‡∏≠‡∏•‡πÑ‡∏ó‡∏¢ ‡∏ü‡∏∏‡∏ï‡∏ö‡∏≠‡∏•‡∏ó‡∏µ‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÑ‡∏ó‡∏¢ ‡πÑ‡∏ó‡∏¢‡∏•‡∏µ‡∏Å ‡∏ü‡∏∏‡∏ï‡∏ã‡∏≠‡∏•‡πÑ‡∏ó‡∏¢ Carabao 7-a-Side Cup ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°/‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ‡∏ö‡∏µ ‡∏ö‡∏≤‡∏á‡∏õ‡∏∞‡∏Å‡∏á ‡∏™‡∏ô‡∏≤‡∏°‡∏Å‡∏µ‡∏¨‡∏≤‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÑ‡∏ü‡∏ï‡πå‡∏™‡∏õ‡∏≠‡∏£‡πå‡∏ï ‡∏°‡∏ß‡∏¢‡πÑ‡∏ó‡∏¢ ‡∏°‡∏ß‡∏¢‡πÇ‡∏•‡∏Å ‡∏Å‡∏µ‡∏¨‡∏≤‡πÇ‡∏•‡∏Å ", "", txt)
    txt = re.sub(r"‡∏ß‡∏≠‡∏•‡πÄ‡∏•‡∏¢‡πå‡∏ö‡∏≠‡∏• ‡πÅ‡∏ö‡∏î‡∏°‡∏¥‡∏ô‡∏ï‡∏±‡∏ô ‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏õ‡∏≠‡∏£‡πå‡∏ï ‡∏Å‡∏µ‡∏¨‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ ‡πÑ‡∏Æ‡πÑ‡∏•‡∏ï‡πå ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏ö‡∏Ç‡∏≠‡∏ö‡∏™‡∏ô‡∏≤‡∏° ‡πÑ‡∏ó‡∏¢‡∏£‡∏±‡∏ê‡πÄ‡∏•‡πà‡∏≤‡∏Å‡∏µ‡∏¨‡∏≤ sport daily ‡πÄ‡∏ä‡∏µ‡∏¢‡∏£‡πå‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡∏Å‡∏∂‡∏Å‡∏Å‡πâ‡∏≠‡∏á ‡πÅ‡∏Å‡∏•‡πÄ‡∏•‡∏≠‡∏£‡∏µ‡πà ‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å ‡∏ü‡∏∏‡∏ï‡∏ö‡∏≠‡∏•‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® ‡∏û‡∏£‡∏µ‡πÄ‡∏°‡∏µ‡∏¢‡∏£‡πå‡∏•‡∏µ‡∏Å ‡∏¢‡∏π‡∏ü‡πà‡∏≤ ‡πÅ‡∏ä‡∏°‡πÄ‡∏õ‡∏µ‡∏¢‡∏ô‡∏™‡πå‡∏•‡∏µ‡∏Å ‡∏ö‡∏∏‡∏ô‡πÄ‡∏î‡∏™‡∏•‡∏µ‡∏Å‡∏≤ ‡∏Å‡∏±‡∏•‡πÇ‡∏ä ‡πÄ‡∏ã‡πÄ‡∏£‡∏µ‡∏¢ ‡∏≠‡∏≤ ‡∏•‡∏≤ ‡∏•‡∏µ‡∏Å‡∏≤ ‡πÄ‡∏à‡∏•‡∏µ‡∏Å ‡∏•‡∏µ‡∏Å‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ", "", txt)
    txt = re.sub(r"‡∏ß‡∏≠‡∏•‡πÄ‡∏•‡∏¢‡πå‡∏ö‡∏≠‡∏• ‡πÅ‡∏ö‡∏î‡∏°‡∏¥‡∏ô‡∏ï‡∏±‡∏ô ‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏õ‡∏≠‡∏£‡πå‡∏ï ‡∏Å‡∏µ‡∏¨‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ ‡πÑ‡∏Æ‡πÑ‡∏•‡∏ï‡πå ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏ö‡∏Ç‡∏≠‡∏ö‡∏™‡∏ô‡∏≤‡∏° ‡πÑ‡∏ó‡∏¢‡∏£‡∏±‡∏ê‡πÄ‡∏•‡πà‡∏≤‡∏Å‡∏µ‡∏¨‡∏≤ sport daily ‡πÄ‡∏ä‡∏µ‡∏¢‡∏£‡πå‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡∏Å‡∏∂‡∏Å‡∏Å‡πâ‡∏≠‡∏á ‡πÅ‡∏Å‡∏•‡πÄ‡∏•‡∏≠‡∏£‡∏µ‡πà ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà THAIRATH ON", "", txt)
    
    # 4) normalize ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ (‡πÄ‡∏ä‡πà‡∏ô ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ã‡πâ‡∏≠‡∏ô / ‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô)
    txt = unicodedata.normalize("NFC", txt)
    txt = normalize(txt)

    # 5) ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥
    txt = re.sub(r"\s+", " ", txt).strip()

    # 6) ‡∏Ñ‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ä‡πà‡∏ß‡∏¢ segmentation ‡πÄ‡∏ä‡πà‡∏ô ‡∏à‡∏∏‡∏î / ‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö / ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏û‡∏π‡∏î
    txt = re.sub(r"([?!])", r" \1 ", txt)
    txt = re.sub(r"([()\"‚Äú‚Äù‚Äò‚Äô])", r" \1 ", txt)
    txt = re.sub(r"\s+", " ", txt).strip()

    return txt

# -------------------------------------------------
# Kuy Clean V2
# -------------------------------------------------

def remove_tail_noise(text: str) -> str:
    """
    ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏Ñ‡∏£‡∏î‡∏¥‡∏ï ‡∏≠‡∏±‡∏•‡∏ö‡∏±‡πâ‡∏° ‡πÅ‡∏ä‡∏£‡πå‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ ‡∏Ø‡∏•‡∏Ø
    """
    patterns = [
        r"‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°.*", r"‡∏î‡∏π‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\s*\d+\s*‡∏†‡∏≤‡∏û.*", r"‡πÅ‡∏ä‡∏£‡πå‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ.*",
        r"‡∏Ç‡∏≠‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì.*", r"‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô.*", r"Facebook.*", r"Twitter.*",
        r"LINE.*", r"‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÇ‡∏ã‡πÄ‡∏ä‡∏µ‡∏¢‡∏•.*", r"‡πÄ‡∏°‡πâ‡∏≤‡∏ó‡πå‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏°‡∏∑‡∏≠‡∏á.*"
    ]
    for p in patterns:
        text = re.split(p, text)[0]
    return text.strip()

def clean_textv2(txt: str) -> str:
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô labeling"""
    if not txt:
        return ""
    # ‡∏•‡∏ö HTML tag ‡πÅ‡∏•‡∏∞ emoji
    txt = re.sub(r"<[^>]+>", " ", txt)
    txt = re.sub(r"[\U00010000-\U0010ffff]", "", txt)

    # Normalize ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ (‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå, ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ã‡πâ‡∏≠‡∏ô)
    txt = unicodedata.normalize("NFC", txt)
    txt = normalize(txt)

    # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡∏ã‡πâ‡∏≥‡πÜ ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô
    txt = re.sub(r"[\"‚Äú‚Äù‚Äò‚Äô]+", "", txt)
    txt = re.sub(r"\s{2,}", " ", txt)
    
    # CLEAN FUCKING STUPID WORDS
    txt = re.sub(" ‡∏û‡∏¥‡∏°‡∏û‡πå", " ", txt)
    txt = re.sub("&nbsp;", " ", txt)
    txt = re.sub(r"\s*\+\s*", " ", txt)

    txt = re.sub(r"TAGS:.*$", "", txt, flags=re.MULTILINE)
    txt = re.sub(r"‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á.*$", "", txt, flags=re.MULTILINE)
    txt = re.sub(r"‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô:.*$", "", txt, flags=re.MULTILINE)
    txt = re.sub(r"‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á .*$", "", txt, flags=re.MULTILINE)

    # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ï‡∏Å‡πÅ‡∏ï‡πà‡∏á ( - ‡∏Å ‡∏Å + ) ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß
    txt = re.sub(r"-\s*‡∏Å\s*‡∏Å\s*\+", "", txt)

    # ‡∏•‡∏ö timestamp, ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö
    txt = re.sub(r"\d{1,2}\s*[‡∏Å-‡∏Æ]+\.\s*\d{2,4}", "", txt)
    txt = re.sub(r"\(.*?‡∏ô\.\)", "", txt)

    # ‡∏•‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß
    txt = remove_tail_noise(txt)

    # ‡∏•‡∏ö space ‡∏ã‡πâ‡∏≥‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
    txt = re.sub(r"\s+", " ", txt).strip()

    return txt

# -------------------------------------------------
# SAVE DATA
# -------------------------------------------------

def save_news_log(clean_text: str, summary_text: str, url_link: str):
    logs_dir = Path("logs")
    logs_dir.mkdir(parents=True, exist_ok=True)

    existing_files = list(logs_dir.glob("*.json"))
    next_number = len(existing_files) + 1

    filename = logs_dir / f"news_{next_number}.json"

    data = {
        "url": url_link,
        "clean_text": clean_text,
        "summary_text": summary_text
    }

    with filename.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)


# -------------------------------------------------
# Routes
# -------------------------------------------------
@app.route("/", methods=["GET", "POST"])
def index():
    if request.method == "POST":
        url = (request.form.get("url") or "").strip()
        n_sent = int(request.form.get("n_sent") or 5)

        if not url:
            return render_template("index.html", error="‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ç‡πà‡∏≤‡∏ß")

        try:
            
            full_text = fetch_full(url)
            full_text = preprocess_for_inference(full_text) 
            full_text = clean_text(full_text)
            full_text = clean_textv2(full_text)
            if len(full_text) < 120:
                return render_template("index.html", error="‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡πÑ‡∏°‡πà‡∏û‡∏≠ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏•‡∏≠‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏≠‡∏∑‡πà‡∏ô")

            summary_text = summarize_th(full_text, n_sent=n_sent)
            highlighted_html, ent_table, totalf1 = highlight_entities(summary_text)
            save_news_log(fetch_full(url), summary_text, url)
            
            f1_scores = {}
            for label in ent_table.keys():
                f1_scores[label] = 0.5
            
            return render_template(
                "result.html",
                url=url,
                summary_html=highlighted_html,
                cleantxt=fetch_full(url),
                ent_table=ent_table,
                f1_scores=f1_scores,
                totalScore=totalf1,
                full_char=len(full_text),
                sum_char=len(summary_text),
            )

        except Exception as e:
            return render_template("index.html", error=f"‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")

    return render_template("index.html")

if __name__ == "__main__":
    app.run()