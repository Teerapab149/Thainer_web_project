# auto_label_hf.py (fixed)
import re, json
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

INPUT_FILE = Path("data/ready_for_label_soft.jsonl")
OUTPUT_FILE = Path("data/hf_labeled_news.jsonl")

MODEL_NAME = "pythainlp/thainer-corpus-v2-base-model"  # ‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
print(f"üîπ Loading model: {MODEL_NAME}")

tok = AutoTokenizer.from_pretrained(MODEL_NAME)
mdl = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)

nlp_ner = pipeline(
    "ner",
    model=mdl,
    tokenizer=tok,
    aggregation_strategy="simple",  # ‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞ dedupe ‡∏î‡πâ‡∏ß‡∏¢ span
)

# Thai date/era ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ç‡∏∂‡πâ‡∏ô
regex_rules = {
    "DATE": re.compile(
        r"(?:\d{1,2}\s?(?:‡∏°\.‡∏Ñ\.|‡∏Å\.‡∏û\.|‡∏°‡∏µ\.‡∏Ñ\.|‡πÄ‡∏°\.‡∏¢\.|‡∏û\.‡∏Ñ\.|‡∏°‡∏¥\.‡∏¢\.|‡∏Å\.‡∏Ñ\.|‡∏™\.‡∏Ñ\.|‡∏Å\.‡∏¢\.|‡∏ï\.‡∏Ñ\.|‡∏û\.‡∏¢\.|‡∏ò\.‡∏Ñ\.|"
        r"‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå|‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°|‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô|‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°|‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô|‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°|‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô|‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°|‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô|‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°)"
        r"(?:\s?\d{2,4})?|‡∏û\.‡∏®\.\s?\d{4}|‡∏Ñ\.‡∏®\.\s?\d{4})"
    ),
    "TIME": re.compile(r"\d{1,2}:\d{2}\s?(?:‡∏ô\.|am|pm|AM|PM)?"),
    "PERCENT": re.compile(r"\d+(?:\.\d+)?%"),
    "MONEY": re.compile(r"\d{1,3}(?:,\d{3})*(?:\.\d+)?\s?(?:‡∏ö‡∏≤‡∏ó|‡∏î‡∏≠‡∏•‡∏•‡∏≤‡∏£‡πå|USD|THB)"),
}

def chunk(text, max_len=350):
    sents = re.split(r'(?<=[.!?‚Ä¶‚Äú‚Äù\n])', text)
    cur, out = "", []
    for s in sents:
        if len(cur) + len(s) <= max_len:
            cur += s
        else:
            if cur.strip():
                out.append(cur.strip())
            cur = s
    if cur.strip():
        out.append(cur.strip())
    return out

def clean_word(w):
    w = (w or "").strip().replace("\u200b", "").replace("\xa0", "")
    if not w or re.fullmatch(r"[\(\)\[\]\-‚Äì‚Äî.,\"'¬´¬ª\s]+", w):
        return None
    return w

def label_text(text):
    ents = []
    used = set()  # span-based de-dup: (start,end,label,word)

    # HF
    offset = 0
    for ch in chunk(text):
        try:
            res = nlp_ner(ch)
            for e in res:
                word = clean_word(e["word"])
                if not word:
                    continue
                # ‡∏´‡∏≤ span ‡∏ï‡∏£‡∏á ‡πÜ ‡πÉ‡∏ô‡∏ä‡∏¥‡πâ‡∏ô chunk
                for m in re.finditer(re.escape(word), ch):
                    s, t = offset + m.start(), offset + m.end()
                    key = (s, t, e["entity_group"], word)
                    if key in used:
                        continue
                    ents.append({"entity": e["entity_group"], "word": word, "score": float(e["score"]), "start": s, "end": t})
                    used.add(key)
        except Exception as ex:
            pass
        offset += len(ch)

    # Regex
    for tag, patt in regex_rules.items():
        for m in patt.finditer(text):
            s, t = m.start(), m.end()
            word = text[s:t]
            key = (s, t, tag, word)
            if key in used:
                continue
            ents.append({"entity": tag, "word": word, "score": 1.0, "start": s, "end": t})
            used.add(key)

    # ‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏õ‡πá‡∏ô entities (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Ñ‡∏µ‡∏¢‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏™‡πÄ‡∏ï‡πá‡∏õ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)
    return ents

def main():
    if not INPUT_FILE.exists():
        print("‚ùå missing input")
        return
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)

    with INPUT_FILE.open("r", encoding="utf-8") as fi, OUTPUT_FILE.open("w", encoding="utf-8") as fo:
        for line in fi:
            obj = json.loads(line)
            text = (obj.get("text") or "").strip()
            if not text:
                continue
            entities = label_text(text)
            obj["entities"] = entities  # ‚Üê ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô labels ‚Üí entities ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            fo.write(json.dumps(obj, ensure_ascii=False) + "\n")
    print(f"‚úÖ wrote: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
