{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd435a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å: data\\t_news.jsonl\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\t_news.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéØ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏Ñ‡∏•‡∏µ‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkept\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° ‚Üí \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müßπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å:\u001b[39m\u001b[33m\"\u001b[39m, input_file)\n\u001b[32m     68\u001b[39m total, kept = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43minput_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin, output_file.open(\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fout:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m fin:\n\u001b[32m     72\u001b[39m         total += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py:537\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    536\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data\\\\t_news.jsonl'"
     ]
    }
   ],
   "source": [
    "# t_pre_clean.py (for t_news.jsonl)\n",
    "import json, re, unicodedata\n",
    "from pathlib import Path\n",
    "from pythainlp.util import normalize\n",
    "\n",
    "# ---------- PATH ----------\n",
    "input_file = Path(\"data/t_news.jsonl\")       # ‚Üê ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å prepare_data.py\n",
    "output_file = Path(\"data/cleaned_news.jsonl\")\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ----------\n",
    "def is_valid(text: str) -> bool:\n",
    "    \"\"\"‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö labeling\"\"\"\n",
    "    if len(text) < 80:\n",
    "        return False\n",
    "    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ URL ‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ clean\n",
    "    if text.count(\"http\") > 1:\n",
    "        return False\n",
    "    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 30% ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å\n",
    "    th_chars = len(re.findall(r\"[\\u0E00-\\u0E7F]\", text))\n",
    "    if th_chars / max(len(text), 1) < 0.3:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# ---------- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô clean ----------\n",
    "def clean_text(txt: str) -> str:\n",
    "    \"\"\"‡∏•‡πâ‡∏≤‡∏á HTML, ‡∏≠‡∏µ‡πÇ‡∏°‡∏à‡∏¥, ‡∏Ç‡∏¢‡∏∞‡∏ó‡πâ‡∏≤‡∏¢‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥\"\"\"\n",
    "    if not txt:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) ‡πÄ‡∏≠‡∏≤‡πÅ‡∏ó‡πá‡∏Å HTML ‡∏≠‡∏≠‡∏Å\n",
    "    txt = re.sub(r\"<[^>]+>\", \" \", txt)\n",
    "\n",
    "    # 2) ‡∏•‡∏ö emoji ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ô‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á Unicode ‡πÑ‡∏ó‡∏¢/‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©\n",
    "    txt = re.sub(r\"[\\U00010000-\\U0010ffff]\", \"\", txt)\n",
    "\n",
    "    # 3) ‡∏•‡∏ö‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß\n",
    "    txt = re.sub(r\"‡∏û‡∏¥‡∏°‡∏û‡πå ‡πÅ‡∏ä‡∏£‡πå‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ ‡πÅ‡∏ä‡∏£‡πå‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ Line Twitter Facebook ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå - ‡∏Å ‡∏Å\", \"\", txt)\n",
    "    txt = re.sub(r\"(‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡∏ó‡∏µ‡πà.*|‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ä‡∏°‡∏†‡∏≤‡∏û.*|‡∏î‡∏π‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°.*|‡πÄ‡∏Ñ‡∏£‡∏î‡∏¥‡∏ï‡∏†‡∏≤‡∏û.*)\", \"\", txt)\n",
    "    txt = re.sub(r\"(appeared first on .*|The post .*)\", \"\", txt, flags=re.I)\n",
    "    txt = re.sub(r\"(Facebook.*?Twitter.*?LINE)\", \"\", txt)\n",
    "    txt = re.sub(r\"&#82\\d{2};\", \"\", txt)\n",
    "    txt = re.sub(r\"\\s*\\[\\]\\s*\", \" \", txt)\n",
    "    txt = re.sub(r\"‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö:.*$\", \"\", txt)\n",
    "    txt = re.sub(r\"Thairath Online Thairath Money Thairath Shopping Thairath Plus Thairath TV MIRROR ‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å ‡∏ü‡∏∏‡∏ï‡∏ö‡∏≠‡∏•‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® ‡∏û‡∏£‡∏µ‡πÄ‡∏°‡∏µ‡∏¢‡∏£‡πå‡∏•‡∏µ‡∏Å ‡∏¢‡∏π‡∏ü‡πà‡∏≤ ‡πÅ‡∏ä‡∏°‡πÄ‡∏õ‡∏µ‡∏¢‡∏ô‡∏™‡πå‡∏•‡∏µ‡∏Å ‡∏ö‡∏∏‡∏ô‡πÄ‡∏î‡∏™‡∏•‡∏µ‡∏Å‡∏≤ ‡∏Å‡∏±‡∏•‡πÇ‡∏ä ‡πÄ‡∏ã‡πÄ‡∏£‡∏µ‡∏¢ ‡∏≠‡∏≤ ‡∏•‡∏≤ ‡∏•‡∏µ‡∏Å‡∏≤ ‡πÄ‡∏à‡∏•‡∏µ‡∏Å ‡∏•‡∏µ‡∏Å‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô\", \"\", txt)\n",
    "    txt = re.sub(r\"‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°/‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô ‡∏î‡∏≤‡∏ß‡∏ã‡∏±‡∏•‡πÇ‡∏ß ‡∏ü‡∏∏‡∏ï‡∏ö‡∏≠‡∏•‡πÑ‡∏ó‡∏¢ ‡∏ü‡∏∏‡∏ï‡∏ö‡∏≠‡∏•‡∏ó‡∏µ‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÑ‡∏ó‡∏¢ ‡πÑ‡∏ó‡∏¢‡∏•‡∏µ‡∏Å ‡∏ü‡∏∏‡∏ï‡∏ã‡∏≠‡∏•‡πÑ‡∏ó‡∏¢ Carabao 7-a-Side Cup ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°/‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ‡∏ö‡∏µ ‡∏ö‡∏≤‡∏á‡∏õ‡∏∞‡∏Å‡∏á ‡∏™‡∏ô‡∏≤‡∏°‡∏Å‡∏µ‡∏¨‡∏≤‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÑ‡∏ü‡∏ï‡πå‡∏™‡∏õ‡∏≠‡∏£‡πå‡∏ï ‡∏°‡∏ß‡∏¢‡πÑ‡∏ó‡∏¢ ‡∏°‡∏ß‡∏¢‡πÇ‡∏•‡∏Å ‡∏Å‡∏µ‡∏¨‡∏≤‡πÇ‡∏•‡∏Å \", \"\", txt)\n",
    "    txt = re.sub(r\"‡∏ß‡∏≠‡∏•‡πÄ‡∏•‡∏¢‡πå‡∏ö‡∏≠‡∏• ‡πÅ‡∏ö‡∏î‡∏°‡∏¥‡∏ô‡∏ï‡∏±‡∏ô ‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏õ‡∏≠‡∏£‡πå‡∏ï ‡∏Å‡∏µ‡∏¨‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ ‡πÑ‡∏Æ‡πÑ‡∏•‡∏ï‡πå ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏ö‡∏Ç‡∏≠‡∏ö‡∏™‡∏ô‡∏≤‡∏° ‡πÑ‡∏ó‡∏¢‡∏£‡∏±‡∏ê‡πÄ‡∏•‡πà‡∏≤‡∏Å‡∏µ‡∏¨‡∏≤ sport daily ‡πÄ‡∏ä‡∏µ‡∏¢‡∏£‡πå‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡∏Å‡∏∂‡∏Å‡∏Å‡πâ‡∏≠‡∏á ‡πÅ‡∏Å‡∏•‡πÄ‡∏•‡∏≠‡∏£‡∏µ‡πà ‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å ‡∏ü‡∏∏‡∏ï‡∏ö‡∏≠‡∏•‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® ‡∏û‡∏£‡∏µ‡πÄ‡∏°‡∏µ‡∏¢‡∏£‡πå‡∏•‡∏µ‡∏Å ‡∏¢‡∏π‡∏ü‡πà‡∏≤ ‡πÅ‡∏ä‡∏°‡πÄ‡∏õ‡∏µ‡∏¢‡∏ô‡∏™‡πå‡∏•‡∏µ‡∏Å ‡∏ö‡∏∏‡∏ô‡πÄ‡∏î‡∏™‡∏•‡∏µ‡∏Å‡∏≤ ‡∏Å‡∏±‡∏•‡πÇ‡∏ä ‡πÄ‡∏ã‡πÄ‡∏£‡∏µ‡∏¢ ‡∏≠‡∏≤ ‡∏•‡∏≤ ‡∏•‡∏µ‡∏Å‡∏≤ ‡πÄ‡∏à‡∏•‡∏µ‡∏Å ‡∏•‡∏µ‡∏Å‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô \", \"\", txt)\n",
    "    txt = re.sub(r\"‡∏ß‡∏≠‡∏•‡πÄ‡∏•‡∏¢‡πå‡∏ö‡∏≠‡∏• ‡πÅ‡∏ö‡∏î‡∏°‡∏¥‡∏ô‡∏ï‡∏±‡∏ô ‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏õ‡∏≠‡∏£‡πå‡∏ï ‡∏Å‡∏µ‡∏¨‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ ‡πÑ‡∏Æ‡πÑ‡∏•‡∏ï‡πå ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏ö‡∏Ç‡∏≠‡∏ö‡∏™‡∏ô‡∏≤‡∏° ‡πÑ‡∏ó‡∏¢‡∏£‡∏±‡∏ê‡πÄ‡∏•‡πà‡∏≤‡∏Å‡∏µ‡∏¨‡∏≤ sport daily ‡πÄ‡∏ä‡∏µ‡∏¢‡∏£‡πå‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡∏Å‡∏∂‡∏Å‡∏Å‡πâ‡∏≠‡∏á ‡πÅ‡∏Å‡∏•‡πÄ‡∏•‡∏≠‡∏£‡∏µ‡πà ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà THAIRATH ON\", \"\", txt)\n",
    "    \n",
    "    # 4) normalize ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ (‡πÄ‡∏ä‡πà‡∏ô ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ã‡πâ‡∏≠‡∏ô / ‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô)\n",
    "    txt = unicodedata.normalize(\"NFC\", txt)\n",
    "    txt = normalize(txt)\n",
    "\n",
    "    # 5) ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "\n",
    "    # 6) ‡∏Ñ‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ä‡πà‡∏ß‡∏¢ segmentation ‡πÄ‡∏ä‡πà‡∏ô ‡∏à‡∏∏‡∏î / ‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö / ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏û‡∏π‡∏î\n",
    "    txt = re.sub(r\"([?!])\", r\" \\1 \", txt)\n",
    "    txt = re.sub(r\"([()\\\"‚Äú‚Äù‚Äò‚Äô])\", r\" \\1 \", txt)\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "\n",
    "    return txt\n",
    "\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "def main():\n",
    "    print(\"üßπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å:\", input_file)\n",
    "    total, kept = 0, 0\n",
    "\n",
    "    with input_file.open(encoding=\"utf-8\") as fin, output_file.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            total += 1\n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            text = clean_text(item.get(\"text\", \"\"))\n",
    "            if is_valid(text):\n",
    "                fout.write(json.dumps({\n",
    "                    \"title\": item.get(\"title\", \"\").strip(),\n",
    "                    \"link\": item.get(\"link\", \"\"),\n",
    "                    \"text\": text\n",
    "                }, ensure_ascii=False) + \"\\n\")\n",
    "                kept += 1\n",
    "\n",
    "            if kept % 50 == 0 and kept > 0:\n",
    "                print(f\"   ‚úÖ ‡∏Ñ‡∏•‡∏µ‡∏ô‡πÅ‡∏•‡πâ‡∏ß {kept} ‡∏Ç‡πà‡∏≤‡∏ß (‡∏à‡∏≤‡∏Å {total})\")\n",
    "\n",
    "    print(f\"\\nüéØ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏Ñ‡∏•‡∏µ‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {kept}/{total} ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° ‚Üí {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "583c5db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßº ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° labeling ...\n",
      "   ‚úÖ ‡∏Ñ‡∏•‡∏µ‡∏ô‡πÅ‡∏•‡πâ‡∏ß 50 ‡∏Ç‡πà‡∏≤‡∏ß (‡∏à‡∏≤‡∏Å 50)\n",
      "   ‚úÖ ‡∏Ñ‡∏•‡∏µ‡∏ô‡πÅ‡∏•‡πâ‡∏ß 100 ‡∏Ç‡πà‡∏≤‡∏ß (‡∏à‡∏≤‡∏Å 100)\n",
      "   ‚úÖ ‡∏Ñ‡∏•‡∏µ‡∏ô‡πÅ‡∏•‡πâ‡∏ß 150 ‡∏Ç‡πà‡∏≤‡∏ß (‡∏à‡∏≤‡∏Å 150)\n",
      "   ‚úÖ ‡∏Ñ‡∏•‡∏µ‡∏ô‡πÅ‡∏•‡πâ‡∏ß 200 ‡∏Ç‡πà‡∏≤‡∏ß (‡∏à‡∏≤‡∏Å 220)\n",
      "\n",
      "üéØ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏£‡πâ‡∏≠‡∏° labeling ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 206/226 ‡∏Ç‡πà‡∏≤‡∏ß ‚Üí data\\ready_for_label.jsonl\n"
     ]
    }
   ],
   "source": [
    "# t_pre_clean_v2.py\n",
    "import json, re, unicodedata\n",
    "from pathlib import Path\n",
    "from pythainlp.util import normalize\n",
    "\n",
    "# ---------- PATH ----------\n",
    "input_file = Path(\"data/cleaned_news.jsonl\")   # ‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\n",
    "output_file = Path(\"data/ready_for_label.jsonl\")\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢ ----------\n",
    "def remove_tail_noise(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏Ñ‡∏£‡∏î‡∏¥‡∏ï ‡∏≠‡∏±‡∏•‡∏ö‡∏±‡πâ‡∏° ‡πÅ‡∏ä‡∏£‡πå‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ ‡∏Ø‡∏•‡∏Ø\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r\"‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°.*\", r\"‡∏î‡∏π‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\\s*\\d+\\s*‡∏†‡∏≤‡∏û.*\", r\"‡πÅ‡∏ä‡∏£‡πå‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ.*\",\n",
    "        r\"‡∏Ç‡∏≠‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì.*\", r\"‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô.*\", r\"Facebook.*\", r\"Twitter.*\",\n",
    "        r\"LINE.*\", r\"‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÇ‡∏ã‡πÄ‡∏ä‡∏µ‡∏¢‡∏•.*\", r\"‡πÄ‡∏°‡πâ‡∏≤‡∏ó‡πå‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏°‡∏∑‡∏≠‡∏á.*\"\n",
    "    ]\n",
    "    for p in patterns:\n",
    "        text = re.split(p, text)[0]\n",
    "    return text.strip()\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    \"\"\"‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô labeling\"\"\"\n",
    "    if not txt:\n",
    "        return \"\"\n",
    "    # ‡∏•‡∏ö HTML tag ‡πÅ‡∏•‡∏∞ emoji\n",
    "    txt = re.sub(r\"<[^>]+>\", \" \", txt)\n",
    "    txt = re.sub(r\"[\\U00010000-\\U0010ffff]\", \"\", txt)\n",
    "\n",
    "    # Normalize ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ (‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå, ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ã‡πâ‡∏≠‡∏ô)\n",
    "    txt = unicodedata.normalize(\"NFC\", txt)\n",
    "    txt = normalize(txt)\n",
    "\n",
    "    # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡∏ã‡πâ‡∏≥‡πÜ ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô\n",
    "    txt = re.sub(r\"[\\\"‚Äú‚Äù‚Äò‚Äô]+\", \"\", txt)\n",
    "    txt = re.sub(r\"\\s{2,}\", \" \", txt)\n",
    "    \n",
    "    # CLEAN FUCKING STUPID WORDS\n",
    "    txt = re.sub(\" ‡∏û‡∏¥‡∏°‡∏û‡πå\", \" \", txt)\n",
    "    txt = re.sub(\"&nbsp;\", \" \", txt)\n",
    "    txt = re.sub(r\"\\s*\\+\\s*\", \" \", txt)\n",
    "\n",
    "    txt = re.sub(r\"TAGS:.*$\", \"\", txt, flags=re.MULTILINE)\n",
    "    txt = re.sub(r\"‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á.*$\", \"\", txt, flags=re.MULTILINE)\n",
    "    txt = re.sub(r\"‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô:.*$\", \"\", txt, flags=re.MULTILINE)\n",
    "    txt = re.sub(r\"‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á .*$\", \"\", txt, flags=re.MULTILINE)\n",
    "\n",
    "    # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ï‡∏Å‡πÅ‡∏ï‡πà‡∏á ( - ‡∏Å ‡∏Å + ) ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß\n",
    "    txt = re.sub(r\"-\\s*‡∏Å\\s*‡∏Å\\s*\\+\", \"\", txt)\n",
    "\n",
    "    # ‡∏•‡∏ö timestamp, ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö\n",
    "    txt = re.sub(r\"\\d{1,2}\\s*[‡∏Å-‡∏Æ]+\\.\\s*\\d{2,4}\", \"\", txt)\n",
    "    txt = re.sub(r\"\\(.*?‡∏ô\\.\\)\", \"\", txt)\n",
    "\n",
    "    # ‡∏•‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß\n",
    "    txt = remove_tail_noise(txt)\n",
    "\n",
    "    # ‡∏•‡∏ö space ‡∏ã‡πâ‡∏≥‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "\n",
    "    return txt\n",
    "\n",
    "def is_valid(text: str) -> bool:\n",
    "    \"\"\"‡πÄ‡∏ä‡πá‡∏Å‡∏ß‡πà‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö labeling ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\"\"\"\n",
    "    if len(text) < 100:\n",
    "        return False\n",
    "    th_chars = len(re.findall(r\"[\\u0E00-\\u0E7F]\", text))\n",
    "    if th_chars / max(len(text), 1) < 0.4:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "def main():\n",
    "    print(\"üßº ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° labeling ...\")\n",
    "    total, kept = 0, 0\n",
    "\n",
    "    with input_file.open(encoding=\"utf-8\") as fin, output_file.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            total += 1\n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            text = clean_text(item.get(\"text\", \"\"))\n",
    "            if is_valid(text):\n",
    "                fout.write(json.dumps({\n",
    "                    \"title\": item.get(\"title\", \"\").strip(),\n",
    "                    \"text\": text\n",
    "                }, ensure_ascii=False) + \"\\n\")\n",
    "                kept += 1\n",
    "\n",
    "            if kept % 50 == 0 and kept > 0:\n",
    "                print(f\"   ‚úÖ ‡∏Ñ‡∏•‡∏µ‡∏ô‡πÅ‡∏•‡πâ‡∏ß {kept} ‡∏Ç‡πà‡∏≤‡∏ß (‡∏à‡∏≤‡∏Å {total})\")\n",
    "\n",
    "    print(f\"\\nüéØ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏£‡πâ‡∏≠‡∏° labeling ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {kept}/{total} ‡∏Ç‡πà‡∏≤‡∏ß ‚Üí {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
